1.
cat@cataliar:~/Downloads$ sudo addgroup hadoop
正在添加组"hadoop" (GID 1001)...
完成。
cat@cataliar:~/Downloads$ 
cat@cataliar:~/Downloads$ 
cat@cataliar:~/Downloads$ 
cat@cataliar:~/Downloads$ sudo adduser -ingroup hadoop hadoop
正在添加用户"hadoop"...
正在添加新用户"hadoop" (1001) 到组"hadoop"...
创建主目录"/home/hadoop"...
正在从"/etc/skel"复制文件...
输入新的 UNIX 密码： 
重新输入新的 UNIX 密码： 
passwd：已成功更新密码
正在改变 hadoop 的用户信息
请输入新值，或直接敲回车键以使用默认值
	全名 []: 
	房间号码 []: 
	工作电话 []: 
	家庭电话 []: 
	其它 []: 
这些信息是否正确？ [Y/n] 
cat@cataliar:~/Downloads$ 
cat@cataliar:~/Downloads$ 
cat@cataliar:~/Downloads$ 
cat@cataliar:~/Downloads$ 
cat@cataliar:~/Downloads$ sudo vim /etc/sudoers
cat@cataliar:~/Downloads$ cat /etc/sudoers
cat: /etc/sudoers: 权限不够
cat@cataliar:~/Downloads$ sudo cat /etc/sudoers
#
# This file MUST be edited with the 'visudo' command as root.
#
# Please consider adding local content in /etc/sudoers.d/ instead of
# directly modifying this file.
#
# See the man page for details on how to write a sudoers file.
#
Defaults	env_reset
Defaults	mail_badpass
Defaults	secure_path="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin"

# Host alias specification

# User alias specification

# Cmnd alias specification

# User privilege specification
root	ALL=(ALL:ALL) ALL
hadoop ALL=(ALL:ALL) ALL

# Members of the admin group may gain root privileges
%admin ALL=(ALL) ALL

# Allow members of group sudo to execute any command
%sudo	ALL=(ALL:ALL) ALL

# See sudoers(5) for more information on "#include" directives:

#includedir /etc/sudoers.d
cat@cataliar:~/Downloads$ 

2.
hadoop@cataliar:~$ ssh-keygen -t rsa -P ""
Generating public/private rsa key pair.
Enter file in which to save the key (/home/hadoop/.ssh/id_rsa):    
Created directory '/home/hadoop/.ssh'.
Your identification has been saved in /home/hadoop/.ssh/id_rsa.
Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.
The key fingerprint is:
fb:db:26:b5:4f:3e:8a:cd:65:7b:90:ee:0e:9d:49:e6 hadoop@cataliar
The key's randomart image is:
+--[ RSA 2048]----+
|                 |
|                 |
|                 |
|                 |
|        S     o. |
|         .  .=oo |
|        .  ..oE. |
|         ..=o*o..|
|          ++==*o |
+-----------------+
hadoop@cataliar:~$ 
hadoop@cataliar:~/.ssh$ cp id_rsa.pub authorized_keys
hadoop@cataliar:~/.ssh$ cat mjh_103_hadoop_rsa.pub >> authorized_keys 
hadoop@cataliar:~/.ssh$ cat authorized_keys 
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDf4uBJyMK4YDgVPgKumFJ7aXbXcLYkyFcrmvGzxUHQaem8wtUpf0OnfT8nYU925a3TpA67U5N+45ghD4SwE5xLWckGChHF6qdhojf76cNEfU9sXU/OMoxAgvYVdVDrVObarniJS8gkGl9Tib9LtiZSn1ULb9VzRwsazmM0u8UXVrMUz8atezEmL7uk436Gr1OZkilfAgWNYzUXYsXfRZh2OOkACPNXZQv58Vt1NuuEpzJzBlDjXPXd9r9eE6PBb0htUJrI1wpG30hv767rGbCzZjnUjeB4CksqOE2WqMKnIBRqMMcJmuLQYHRlfIOqL6cXXYL5vX+jLxIEjhYeuxQH hadoop@cataliar
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCkoGJJcte29BRC/LpvMz08bURhYxCMgMcK1nhQB42ym3E0iVTKDxhim9GrebE2tTOoppye9ZdEyP/+y7GECTgPEsm+0WAxwC9L6s9EqarOrU9+6ZfZCoWq1vOBUYR9P04Z4h+e6Zs4s73bTemVVRF7mpI0aG/pZeZOG9Ic0wh7noaPMcgehBSLyFWD861AuwR2ldXrc0sZwJ25ePPYClqj0p8X9z+qRVGrEx/7Cj+dQmuiGIqGzCrukm1UHbaarpE7Io3Q5LICNnYVhhJ6uzutuXzb1dxZjpz1fK390KWGbVXDCkGXFAYEUEh94XbE1pYkgkCAdwee5RCFmPJOt9pJ hadoop@mjh-Vostro-260
hadoop@cataliar:~/.ssh$ 

hadoop@mjh-Vostro-260:~/.ssh$ ls
authorized_keys  id_rsa  id_rsa.pub  known_hosts

3.
hadoop@cataliar:/usr/local/jvm$ sudo tar zxvf jdk-8u151-linux-x64.tar.gz 
hadoop@cataliar:~$ sudo vim /etc/profile
export JAVA_HOME=/usr/local/jvm/jdk1.8.0_151
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:${PATH}

hadoop@cataliar:~$ source /etc/profile
hadoop@cataliar:~$ java -version
java version "1.8.0_151"
Java(TM) SE Runtime Environment (build 1.8.0_151-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode)

4. StandAlone Mode
hadoop@mjh-Vostro-260:/home/mjh/Downloads/pkgs$ scp hadoop-2.7.4.tar.gz hadoop@datanode1:/home/hadoop
The authenticity of host 'datanode1 (115.156.135.59)' can't be established.
ECDSA key fingerprint is f8:d0:3a:54:85:c6:6c:99:40:2f:5c:b1:13:93:73:d6.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'datanode1' (ECDSA) to the list of known hosts.
hadoop-2.7.4.tar.gz                           100%

hadoop@mjh-Vostro-260:/usr/local$ sudo chown -R hadoop:hadoop /usr/local/hadoop/
hadoop@mjh-Vostro-260:/usr/local$ ls -al
total 64
drwxr-xr-x 14 root   root    4096 11月  2 21:14 .
drwxr-xr-x 10 root   root    4096  7月 23  2014 ..
drwxr-xr-x  3 hadoop hadoop  4096 11月  2 11:29 hadoop

hadoop@mjh-Vostro-260:~$ cat /etc/environment 
PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games"
socks_proxy="socks://127.0.0.1:1080/"
hadoop@mjh-Vostro-260:~$ 

hadoop@mjh-Vostro-260:/usr/local/hadoop/hadoop-2.7.4/etc/hadoop$
# JAVA environment settings
export JAVA_HOME=/home/mjh/Public/jdk1.8.0_111
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH
export MAVEN_HOME=/home/mjh/Documents/app/apache-maven-3.5.0
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.4

hadoop@mjh-Vostro-260:~$ cat .bashrc
export PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin:${MAVEN_HOME}/bin:${HADOOP_HOME}/sbin:${HADOOP_HOME}/bin:/usr/local/go/bin:${PATH}

hadoop@mjh-Vostro-260:/usr/local/hadoop/hadoop-2.7.4/share/hadoop/mapreduce/sources$ hadoop jar hadoop-mapreduce-examples-2.7.4-sources.jar org.apache.hadoop.examples.WordCount /home/hadoop/hadoop/mapred/input /home/hadoop/hadoop/mapred/output
17/11/03 13:22:14 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
17/11/03 13:22:14 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
17/11/03 13:22:14 INFO input.FileInputFormat: Total input paths to process : 0
17/11/03 13:22:14 INFO mapreduce.JobSubmitter: number of splits:0
17/11/03 13:22:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1398691657_0001
17/11/03 13:22:15 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
17/11/03 13:22:15 INFO mapreduce.Job: Running job: job_local1398691657_0001
17/11/03 13:22:15 INFO mapred.LocalJobRunner: OutputCommitter set in config null
17/11/03 13:22:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/11/03 13:22:15 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/11/03 13:22:15 INFO mapred.LocalJobRunner: Waiting for map tasks
17/11/03 13:22:15 INFO mapred.LocalJobRunner: map task executor complete.
17/11/03 13:22:15 INFO mapred.LocalJobRunner: Waiting for reduce tasks
17/11/03 13:22:15 INFO mapred.LocalJobRunner: Starting task: attempt_local1398691657_0001_r_000000_0
17/11/03 13:22:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/11/03 13:22:15 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/11/03 13:22:15 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@360afa0e
17/11/03 13:22:15 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10
17/11/03 13:22:15 INFO reduce.EventFetcher: attempt_local1398691657_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
17/11/03 13:22:15 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
17/11/03 13:22:15 INFO mapred.LocalJobRunner: 
17/11/03 13:22:15 INFO reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 0 on-disk map-outputs
17/11/03 13:22:15 INFO reduce.MergeManagerImpl: Merging 0 files, 0 bytes from disk
17/11/03 13:22:15 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
17/11/03 13:22:15 INFO mapred.Merger: Merging 0 sorted segments
17/11/03 13:22:15 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes
17/11/03 13:22:15 INFO mapred.LocalJobRunner: 
17/11/03 13:22:15 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
17/11/03 13:22:15 INFO mapred.Task: Task:attempt_local1398691657_0001_r_000000_0 is done. And is in the process of committing
17/11/03 13:22:15 INFO mapred.LocalJobRunner: 
17/11/03 13:22:15 INFO mapred.Task: Task attempt_local1398691657_0001_r_000000_0 is allowed to commit now
17/11/03 13:22:15 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1398691657_0001_r_000000_0' to file:/home/hadoop/hadoop/mapred/output/_temporary/0/task_local1398691657_0001_r_000000
17/11/03 13:22:15 INFO mapred.LocalJobRunner: reduce > reduce
17/11/03 13:22:15 INFO mapred.Task: Task 'attempt_local1398691657_0001_r_000000_0' done.
17/11/03 13:22:15 INFO mapred.LocalJobRunner: Finishing task: attempt_local1398691657_0001_r_000000_0
17/11/03 13:22:15 INFO mapred.LocalJobRunner: reduce task executor complete.
17/11/03 13:22:16 INFO mapreduce.Job: Job job_local1398691657_0001 running in uber mode : false
17/11/03 13:22:16 INFO mapreduce.Job:  map 0% reduce 100%
17/11/03 13:22:16 INFO mapreduce.Job: Job job_local1398691657_0001 completed successfully
17/11/03 13:22:16 INFO mapreduce.Job: Counters: 24
	File System Counters
		FILE: Number of bytes read=297656
		FILE: Number of bytes written=589483
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=0
		Reduce input records=0
		Reduce output records=0
		Spilled Records=0
		Shuffled Maps =0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=98041856
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=8

5. Pesudo Distributed FS Mode
mjh@mjh-Vostro-260:~/Downloads/pkgs$ ifconfig
eth0      Link encap:Ethernet  HWaddr 78:45:c4:13:3d:2f  
          inet addr:115.156.132.71  Bcast:115.156.135.255  Mask:255.255.252.0
hadoop@cataliar:/usr/local/jvm$ ifconfig
eth0      Link encap:以太网  硬件地址 78:45:c4:27:d2:f1  
          inet 地址:115.156.135.59  广播:115.156.135.255  掩码:255.255.252.0

hadoop@mjh-Vostro-260:/usr/local/hadoop/hadoop-2.7.4/etc/hadoop$ cat /etc/hosts
127.0.0.1	localhost
#127.0.1.1	mjh-Vostro-260
115.156.132.71  mjh-Vostro-260
115.156.135.59  cataliar

hadoop@mjh-Vostro-260:/usr/local/hadoop/hadoop-2.7.4/etc/hadoop$ sudo vim hadoop-env.sh
# The java implementation to use.
export JAVA_HOME=/home/mjh/Public/jdk1.8.0_111

hadoop@mjh-Vostro-260:/usr/local/hadoop/hadoop-2.7.4/etc/hadoop$ cat core-site.xml
<configuration>
<property>
<name>hadoop.tmp.dir</name>
<value>file:/home/hadoop/hadoop/tmp</value>
<description>Abase for other temporary directories.</description>
</property>
<property>
<name>fs.defaultFS</name>
<value>hdfs://localhost:9000</value>
</property>
</configuration>
hadoop@mjh-Vostro-260:/usr/local/hadoop/hadoop-2.7.4/etc/hadoop$ 

hadoop@mjh-Vostro-260:/usr/local/hadoop/hadoop-2.7.4/etc/hadoop$ cat hdfs-site.xml
<configuration>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
<property>
<name>dfs.namenode.name.dir</name>
<value>file:/home/hadoop/hadoop/dfs/name</value>
<description>Path on the local filesystem where the NameNode stores the namespace and transactions logs persistently.</description>
</property>
<property>
<name>dfs.datanode.data.dir</name>
<value>file:/home/hadoop/hadoop/dfs/data</value>
<description>Comma separated list of paths on the local filesystem of a DataNode where it should store its blocks.</description>
</property>
<property>  
<name>dfs.webhdfs.enabled</name>  
<value>true</value>  
</property> 
</configuration>
hadoop@mjh-Vostro-260:/usr/local/hadoop/hadoop-2.7.4/etc/hadoop$ 

hadoop@mjh-Vostro-260:~$ hdfs namenode -format
17/11/03 15:40:05 INFO namenode.NameNode: STARTUP_MSG: 
************************************************************/
17/11/03 15:40:05 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
17/11/03 15:40:05 INFO namenode.NameNode: createNameNode [-format]
Formatting using clusterid: CID-536dc8ed-6084-40eb-9e0e-ad17471620c3
17/11/03 15:40:06 INFO namenode.FSNamesystem: No KeyProvider found.
17/11/03 15:40:06 INFO namenode.FSNamesystem: fsLock is fair: true
17/11/03 15:40:06 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
17/11/03 15:40:06 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
17/11/03 15:40:06 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
17/11/03 15:40:06 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
17/11/03 15:40:06 INFO blockmanagement.BlockManager: The block deletion will start around 2017 Nov 03 15:40:06
17/11/03 15:40:06 INFO util.GSet: Computing capacity for map BlocksMap
17/11/03 15:40:06 INFO util.GSet: VM type       = 64-bit
17/11/03 15:40:06 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB
17/11/03 15:40:06 INFO util.GSet: capacity      = 2^21 = 2097152 entries
17/11/03 15:40:06 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
17/11/03 15:40:06 INFO blockmanagement.BlockManager: defaultReplication         = 1
17/11/03 15:40:06 INFO blockmanagement.BlockManager: maxReplication             = 512
17/11/03 15:40:06 INFO blockmanagement.BlockManager: minReplication             = 1
17/11/03 15:40:06 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
17/11/03 15:40:06 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
17/11/03 15:40:06 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
17/11/03 15:40:06 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
17/11/03 15:40:06 INFO namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
17/11/03 15:40:06 INFO namenode.FSNamesystem: supergroup          = supergroup
17/11/03 15:40:06 INFO namenode.FSNamesystem: isPermissionEnabled = true
17/11/03 15:40:06 INFO namenode.FSNamesystem: HA Enabled: false
17/11/03 15:40:06 INFO namenode.FSNamesystem: Append Enabled: true
17/11/03 15:40:07 INFO util.GSet: Computing capacity for map INodeMap
17/11/03 15:40:07 INFO util.GSet: VM type       = 64-bit
17/11/03 15:40:07 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB
17/11/03 15:40:07 INFO util.GSet: capacity      = 2^20 = 1048576 entries
17/11/03 15:40:07 INFO namenode.FSDirectory: ACLs enabled? false
17/11/03 15:40:07 INFO namenode.FSDirectory: XAttrs enabled? true
17/11/03 15:40:07 INFO namenode.FSDirectory: Maximum size of an xattr: 16384
17/11/03 15:40:07 INFO namenode.NameNode: Caching file names occuring more than 10 times
17/11/03 15:40:07 INFO util.GSet: Computing capacity for map cachedBlocks
17/11/03 15:40:07 INFO util.GSet: VM type       = 64-bit
17/11/03 15:40:07 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB
17/11/03 15:40:07 INFO util.GSet: capacity      = 2^18 = 262144 entries
17/11/03 15:40:07 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
17/11/03 15:40:07 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
17/11/03 15:40:07 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
17/11/03 15:40:07 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
17/11/03 15:40:07 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
17/11/03 15:40:07 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
17/11/03 15:40:07 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
17/11/03 15:40:07 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
17/11/03 15:40:07 INFO util.GSet: Computing capacity for map NameNodeRetryCache
17/11/03 15:40:07 INFO util.GSet: VM type       = 64-bit
17/11/03 15:40:07 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
17/11/03 15:40:07 INFO util.GSet: capacity      = 2^15 = 32768 entries
17/11/03 15:40:07 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1350377035-115.156.132.71-1509694807409
17/11/03 15:40:07 INFO common.Storage: Storage directory /home/hadoop/hadoop/dfs/name has been successfully formatted.
17/11/03 15:40:07 INFO namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
17/11/03 15:40:07 INFO namenode.FSImageFormatProtobuf: Image file /home/hadoop/hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 323 bytes saved in 0 seconds.
17/11/03 15:40:07 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
17/11/03 15:40:07 INFO util.ExitUtil: Exiting with status 0
17/11/03 15:40:07 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at mjh-Vostro-260/115.156.132.71
************************************************************/

hadoop@mjh-Vostro-260:~$ start-dfs.sh
Starting namenodes on [localhost]
localhost: starting namenode, logging to /usr/local/hadoop/hadoop-2.7.4/logs/hadoop-hadoop-namenode-mjh-Vostro-260.out
localhost: starting datanode, logging to /usr/local/hadoop/hadoop-2.7.4/logs/hadoop-hadoop-datanode-mjh-Vostro-260.out
Starting secondary namenodes [0.0.0.0]
The authenticity of host '0.0.0.0 (0.0.0.0)' can't be established.
ECDSA key fingerprint is c3:6a:35:ab:16:6f:00:87:d1:7b:e5:db:6c:01:5b:99.
Are you sure you want to continue connecting (yes/no)? yes
0.0.0.0: Warning: Permanently added '0.0.0.0' (ECDSA) to the list of known hosts.
0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/hadoop-2.7.4/logs/hadoop-hadoop-secondarynamenode-mjh-Vostro-260.out
hadoop@mjh-Vostro-260:~$ 

hadoop@mjh-Vostro-260:/usr/local/hadoop/hadoop-2.7.4/etc/hadoop$ jps
4400 NameNode
4569 DataNode
6234 Jps
4847 SecondaryNameNode

 pesudo-dis filesystem: http://localhost:50070 

hadoop@mjh-Vostro-260:/usr/local/hadoop/hadoop-2.7.4/sbin$ stop-dfs.sh
Stopping namenodes on [localhost]
localhost: stopping namenode
localhost: stopping datanode
Stopping secondary namenodes [0.0.0.0]
0.0.0.0: stopping secondarynamenode















